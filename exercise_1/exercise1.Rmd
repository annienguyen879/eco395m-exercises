---
title: 'ECO 395 Homework 1:'
author: "Soo Jee Choi, Annie Nguyen, and Tarini Sudhakar"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## 1) Data Visualization: Flights at ABIA

### Introduction

In this section, we are looking at a 2008 dataset that contains information on every commercial flight coming in and out of the Austin-Bergstrom Internationial Airport. We are interested in seeing which cities in the dataset have the highest average departure delays.

### Methods

To explore the data and create visualizations, we first need to import the `tidyverse` and `ggplot2` packages and read the `ABIA.csv` file which contains the data. Since we are also interested in visualizing the data on a geographical map, we will also import the `ggmap` package and read the `airport-codes` files, which contain the latitude and longitude reading of each airport. 

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
library(ggmap)
abia <- read.csv("ABIA.csv")
airport_codes <- read.csv("airport-codes.csv")
```

First, the `airport_codes` dataframe is cleaned to exclude closed airports and drop rows with NA's or no available IATA-codes. Then, the `airport_codes` dataframe is joined with `abia` dataframe, so each US airport in our targeted dataframe has a respective value for longitude and latitude.

Then, we calculate the average departure delay from each Origin airport. Airports with the highest departure delays include: Knoxville, TN; Birmingham, AL; Washington, D.C.; Raleigh/Durham, NC; San Antonio, TX; Philadelphia, PA.


```{r}
airport_codes <- airport_codes %>% 
  filter(type != "closed", iata_code != "") %>%
  separate(col = coordinates, into= c("lat", "long"), sep = ",") %>%  
  transform(lat = as.numeric(lat), long = as.numeric(long)) %>% 
  select(iata_code, lat, long) %>% drop_na %>% distinct 

avg_delay <- abia %>% group_by(Origin) %>% 
  summarise(avg_delay_time = mean(DepDelay, na.rm = TRUE)) %>%
  arrange(-avg_delay_time) %>% 
  left_join(airport_codes, by= c("Origin" = "iata_code"))

avg_delay %>% select(Origin, avg_delay_time) %>% head %>%
  knitr::kable(caption = "Top 6 Cities with Highest Departure Delays")
```

We will use a barplot to visualize a larger portion of the dataset, looking at the top 20 cities with the highest departure delays.

From the plot, we see that Knoxville has, on average, significantly higher delays than Birmingham (with departure delays close to 70 minutes for Knoxville and near 40 minutes for Birmingham). In comparison, the average delays for the remaining cities stay below 30 minutes.

```{r}
avg_delay %>% head(20) %>% 
  ggplot(aes(reorder(Origin, avg_delay_time), avg_delay_time)) + 
  geom_bar(stat = "identity", color="white") +   coord_flip() + 
  ggtitle("Top 20 Cities with Highest Departure Delays") + 
  xlab("Origin") + ylab("Average Departure Delays (min)")
```

We will visualize the data for all 53 cities included on the dataset by plotting the points on the US map. This will not only show us average airport delays but also give us a brief look at all of the US cities that flyers can travel to from Austin-Bergstrom Internationial Airport.

```{r}
us_map <- map_data(map = "state")
avg_delay %>% ggplot() + 
  geom_polygon(aes(long, lat, group = group), fill= "white", color="black", data= us_map) + 
  coord_map() + 
  geom_point(aes(long, lat, color = avg_delay_time), size = 2) +
  ggtitle("Average Delay Times Across U.S. Cities")+ 
    scale_color_gradient(low = "orange", high = "navy", name = "Avg. Dep. Delay") + 
  xlab("Longitude") + ylab("Latitude")

```

### Results and Conclusion
Based on the results of the plots, we see that Knoxville and Birmingham experience significantly higher departure delays compared to other cities.

From the data visualization generated in this section, we were able to identify airports that experienced the highest average departure delays. This information can help future flyers know what to expect in terms of departure delays when traveling to Austin.

Of course, this brief data exploration and visualization report comes with its own set of limitations. Departure delays are influenced by holiday rushes and weather conditions, so outliers from unusual weather or from other potential problems may skew results. Furthermore, the dataset only includes observations from 2008 and delays may affect each airport differently on a yearly basis.


## 2) Wrangling the Olympics

### A) What is the 95th percentile of heights for female competitors across all Athletics events (i.e., track and field)?  Note that "sport" is the broad sport (e.g. Athletics) whereas "event" is the specific event (e.g. 100 meter sprint). 

```{r 95 percentile atheltics, message=FALSE, results=FALSE}
library(tidyverse)
olympics_data <- read.csv('olympics_top20.csv')
fem_athletics <- subset(olympics_data, sex=="F" & sport=="Athletics", 
                        select=c(name, sex, sport, height))
quantile(fem_athletics$height, probs = .95)
```

The 95th percentile of heights for female competitors across all Athletics events was obtained by first subsetting the values in the 'olympics_top20.csv' file to observations with the sex variable equal to "F" and the sport variable equal to "Athletic". Then, the quantile function was used to get the 95th quantile of the height of female track and field ("Athletic") Olympians, which was 183cm.


### B) Which single women's "event" had the greatest variability in competitor's heights across the entire history of the Olympics, as measured by the standard deviation?  
```{r women sd height, message=FALSE}
fem_events <- subset(olympics_data, sex=="F", select=c(name, sex, event, height))
fem_height_sd <- fem_events %>% group_by(event) %>% summarise(sd_height = sd(height)) %>% 
  arrange(desc(sd_height))
fem_height_sd %>% select(event, sd_height) %>% head %>%
  knitr::kable(caption = "Top 6 Single Women's Olympic Events with Greatest Variability 
               in Height")
```

To find which single women's event had the greatest variability in competitor's heights (as measured by the standard deviation), we first subset the original data set to contain only females observations. Then the data was grouped by sporting event, and then the standard deviation of the heights for each sport was calculated. Arranging the results in descending order, we find that Rowing Women's Coxed Fours had the greatest variation in competitor's heights (as measured by the standard deviation) at 10.865490.

### C) How has the average age of Olympic swimmers changed over time? Does the trend look different for male swimmers relative to female swimmers?  Create a data frame that can allow you to visualize these trends over time, then plot the data with a line graph with separate lines for male and female competitors.  Give the plot an informative caption answering the two questions just posed. 

```{r Olympic swimmers graph, warning=FALSE, message=FALSE}

#Obtain Olympic swimmers' data by gender
all_swimmers <- subset(olympics_data, sport=="Swimming", select=c(name, sex, sport, age, year))
male_swimmers <- subset(olympics_data, sex=="M" & sport=="Swimming", select=c(name, sex, sport, age, year))
female_swimmers <- subset(olympics_data, sex=="F" & sport=="Swimming", select=c(name, sex, sport, age, year))

#Group by gender and find mean
all_swimmers_mean <- all_swimmers %>% group_by(year) %>% summarise(all_mean_age = mean(age)) %>% arrange(year)
male_swimmers_mean <- male_swimmers %>% group_by(year) %>% summarise(male_mean_age = mean(age)) %>% arrange(year)
female_swimmers_mean <- female_swimmers %>% group_by(year) %>% summarise(female_mean_age = mean(age)) %>% arrange(year)

#Combine datasets
swimmer_data_merge1 <- merge(all_swimmers_mean, male_swimmers_mean, by="year")
swimmer_data_merge2 <- merge(swimmer_data_merge1, female_swimmers_mean, by="year", all=TRUE)

#Plot graph
ggplot(swimmer_data_merge2, aes(x=year)) + 
  ggtitle("Male vs Female: Mean Average Age of Olympic Swimmers by Year") +
  xlab("Year") + ylab("Mean Age") +              
  geom_line(data=swimmer_data_merge2, aes(y = male_mean_age, color = "Male")) + 
  geom_line(data=swimmer_data_merge2, aes(y = female_mean_age, color = "Female")) + 
  geom_line(data=swimmer_data_merge2, aes(y = all_mean_age, color="Both Genders")) +
  scale_color_manual(name = "Key",values = c("Male" = "tomato", "Female" = "steelblue", "Both Genders"="mediumpurple4")) +
  labs(caption=str_wrap("Average age of Olympic swimmers experiences a sharp rise and fall in the early 1900s, and follows 
  an increasing trend thereafter. The average age of male Olympic swimmers generally trends higher than the average age of 
  female Olympic swimmers. However, the average age of male and female Olympic Swimmers notably converge in 2000.")) 
```
The dataset on average age of Olympic swimmers shows that women did not participate in Olympic swimming competitions from 1900 to 1948, with the exception of 1924. We see the average age of Olympic swimmers started at 18 years old in 1900 and reached a peak in 1912 at 27 years old. The data then follows a decreasing trend reaching the lowest average age of 18.53390 in 1976. Then the data follows an increasing trend in the years that follow, with 23.24211 years old in 2016 being the most recent average Olympic swimmers' age data value available.

We see, in the data available for both genders, that male Olympic swimmers, on average, have consistently been older than female Olympic swimmers. However, in 2000, female Olympic swimmer were, on average, 22.53191 years old while male Olympic swimmers were, on average, 22.49451 years old. Hence, we observe a convergence in the graph of the mean average age of Olympic swimmers in the year 2000. Additionally, the average age of male Olympic swimmers experienced fluctuations in the early 1900s (where data on female Olympic swimmers was not available), notably reaching a peak of 32 years old in 1924. Conversely, the data on female Olympic swimmer shows a general increase in average age over time.


## 3) K-Nearest Neighbors: Cars


```{r echo=FALSE}
library(rsample)
library(caret)
library(foreach)
library(modelr)

sclass <- read.csv('sclass.csv')

set.seed(1720104007)

#Filter for 350
car350 <- sclass %>%
  filter(trim == 350)

#Split train/test for 350
car350_split = initial_split(car350, prop=0.8)
car350_train = training(car350_split)
car350_test  = testing(car350_split)
```

## K=2
```{r 350k2}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 2) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base = ggplot(data = car350) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey') + 
  theme_bw(base_size=10) 
p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)

```


## K=5
```{r 350k5}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 5) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)

```


## K=10
```{r 350k10}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 10) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)

```


## K=20
```{r 350k20}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 20) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)

```

## K=30
```{r 350k30}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 30) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)

```


## K=50
```{r 350k50}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 50) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)
```


## K=70
```{r 350k70}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 70) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)
```


## K=100
```{r 350k100}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 100) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)
```

## K=150
```{r 350k150}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 150) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)
```

## K=300
```{r 350k300}
knn_model = knnreg(price ~ mileage, data=car350_train, k = 300) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)
```

## K-nearest neighbors: test  

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

k_grid = unique(round(exp(seq(log(332), log(2), length=300))))
rmse_grid_out = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knnreg(price ~ mileage,
                     data = car350_train, k = k)
  modelr::rmse(knn_model, car350_test)
}

rmse_grid_out = data.frame(K = k_grid, RMSE = rmse_grid_out)

revlog_trans <- function(base = exp(1)) {
  require(scales)
  ## Define the desired transformation.
  trans <- function(x){
    -log(x, base)
  }
  ## Define the reverse of the desired transformation
  inv <- function(x){
    base^(-x)
  }
  ## Creates the transformation
  scales::trans_new(paste("revlog-", base, sep = ""),
                    trans,
                    inv,  ## The reverse of the transformation
                    log_breaks(base = base), ## default way to define the scale breaks
                    domain = c(1e-100, Inf) 
  )
}

p_out = ggplot(data=rmse_grid_out) + 
  theme_bw(base_size = 10) + 
  geom_path(aes(x=K, y=RMSE, color='testset'), size=0.5) + 
  scale_x_continuous(trans=revlog_trans(base = 10)) 

ind_best = which.min(rmse_grid_out$RMSE)
k_best = k_grid[ind_best]

rmse_grid_in = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knnreg(price ~ mileage,
                     data = car350_train, k = k)
  modelr::rmse(knn_model, car350_train)
}

rmse_grid_in = data.frame(K = k_grid, RMSE = rmse_grid_in)

p_out + geom_path(data=rmse_grid_in, aes(x=K, y=RMSE, color='trainset'),size=0.5) +
  scale_colour_manual(name="RMSE",
    values=c(testset="black", trainset="grey")) + 
  geom_vline(xintercept=k_best, color='darkgreen', size=1)
```


## K-nearest neighbors: at the optimal k  

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(price ~ mileage,
                   data = car350_train, k = k_best)

knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car350_test)
```

``` {r knn65}

#Filter for 65 AMG
car65AMG <- sclass %>%
  filter(trim == '65 AMG')

#Split train/test for 65 
car65_split = initial_split(car65AMG, prop=0.8)
car65_train = training(car65_split)
car65_test  = testing(car65_split)

```

## K=2
```{r 65k2}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 2) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base = ggplot(data = car65AMG) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey') + 
  theme_bw(base_size=10) 
p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)

```

## K=5
```{r 65k5}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 5) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)

```


## K=10
```{r 65k10}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 10) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)

```


## K=20
```{r 65k20}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 20) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)

```

## K=30
```{r 65k30}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 30) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)

```


## K=50
```{r 65k50}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 50) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)
```


## K=70
```{r 65k70}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 70) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)
```


## K=100
```{r 65k100}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 100) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)
```

## K=150
```{r 65k150}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 150) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)
```

## K=200
```{r 65k300}
knn_model = knnreg(price ~ mileage, data=car65_train, k = 200) 
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)
```


## K-nearest neighbors: test  

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

k_grid = unique(round(exp(seq(log(233), log(2), length=200))))
rmse_grid_out = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knnreg(price ~ mileage,
                     data = car65_train, k = k)
  modelr::rmse(knn_model, car65_test)
}

rmse_grid_out = data.frame(K = k_grid, RMSE = rmse_grid_out)

p_out = ggplot(data=rmse_grid_out) + 
  theme_bw(base_size = 10) + 
  geom_path(aes(x=K, y=RMSE, color='testset'), size=0.5) + 
  scale_x_continuous(trans=revlog_trans(base = 10)) 

ind_best = which.min(rmse_grid_out$RMSE)
k_best = k_grid[ind_best]

rmse_grid_in = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knnreg(price ~ mileage,
                     data = car65_train, k = k)
  modelr::rmse(knn_model, car65_train)
}

rmse_grid_in = data.frame(K = k_grid, RMSE = rmse_grid_in)

p_out + geom_path(data=rmse_grid_in, aes(x=K, y=RMSE, color='trainset'),size=0.5) +
  scale_colour_manual(name="RMSE",
    values=c(testset="black", trainset="grey")) + 
  geom_vline(xintercept=k_best, color='darkgreen', size=1)
```

## K-nearest neighbors: at the optimal k  

```{r, fig.width=4, fig.asp = 0.6, fig.align='center', echo=FALSE, message=FALSE}
knn_model = knnreg(price ~ mileage,
                   data = car65_train, k = k_best)

knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(mileage=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)

modelr::rmse(knn_model, car65_test)
```

We can see that the trim level of 65 AMG has an optimal k of 12. On the other hand, the 350 trim has an optimal k value of 51. When we have a higher k-value, we will  tend to have a higher bias but lower variance. Prices for cars with the 350 trim level vary less than that of cars with the 65 AMG level. Cars with the 350 trim are mostly clustered between 0 to 50000 miles for a price range between \$50,000 and \$75,000 or below $25,000 when offering mileage between 50,000 and 1,50,000 miles. Here, a higher k-value would be reasonable since we have lower variance within the dataset to be concerned about. 

With a lower k-value, we risk having a higher variance but lower bias. Cars with a 65 AMG level trim are more scattered, and therefore, we will need a lower k-value to make more accurate price predictions. 

The S65 AMG has features of a sports car and thus, will call a lower price on the second-hand market since its working condition will be more critically evaluated than that of a standard sedan. In this case, that is the S350. S65 AMGs with 0 mileage cost around $25,000 while S350 start off with \$50,0000. It may also explain why we have a smaller set of observations for the S65 since there would be a smaller market for a sports car than that for a sedan. The dataset for cars with 350 trim level has 416 observations while the dataset of cars with 65 AMG trim has 292 observations. 

## Checking RMSE through K-cross validation 

###For car 350

```{r}
K_folds = 5

# Pipeline 1:
# create specific fold IDs for each row
# the default behavior of sample actually gives a permutation
Kcar350 = car350 %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(car350)) %>% sample)

head(Kcar350)

# now loop over folds
rmse_cv = foreach(fold = 1:K_folds, .combine='c') %do% {
  knn100 = knnreg(price ~ mileage,
                  data=filter(Kcar350, fold_id != fold), k=100)
  modelr::rmse(knn100, data=filter(Kcar350, fold_id == fold))
}

rmse_cv
mean(rmse_cv)  # mean CV error
sd(rmse_cv)/sqrt(K_folds)   # approximate standard error of CV error

rmse_grid_in = data.frame(K = k_grid, RMSE = rmse_grid_in)

```
###For car 65 AMG

```{r}
K_folds = 5

# Pipeline 1:
# create specific fold IDs for each row
# the default behavior of sample actually gives a permutation
Kcar65 = car65AMG %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(car65AMG)) %>% sample)

head(Kcar65)

# now loop over folds
rmse_cv = foreach(fold = 1:K_folds, .combine='c') %do% {
  knn100 = knnreg(price ~ mileage,
                  data=filter(Kcar65, fold_id != fold), k=100)
  modelr::rmse(knn100, data=filter(Kcar65, fold_id == fold))
}

rmse_cv
mean(rmse_cv)  # mean CV error
sd(rmse_cv)/sqrt(K_folds)   # approximate standard error of CV error

rmse_grid_in = data.frame(K = k_grid, RMSE = rmse_grid_in)

```
```

---
title: "Exercises 2"
author: "Soo Jee, Annie Nguyen, and Tarini Sudhakar"
date: "`r Sys.Date()`"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(rsample)
library(caret)
library(foreach)
library(modelr)
library(wesanderson)
library(pROC)
```

## Saratoga house prices


```{r cars}
summary(cars)
```

## Classification and retrospective sampling
What do you notice about the history variable vis-a-vis predicting defaults? What do you think is going on here? In light of what you see here, do you think this data set is appropriate for building a predictive model of defaults, if the purpose of the model is to screen prospective borrowers to classify them into "high" versus "low" probability of default? Why or why not---and if not, would you recommend any changes to the bank's sampling scheme?

  1. What do you notice about the history variable vis-a-vis predicting defaults? 
  If someone has a poor credit history, then the chance of them defaulting on a loan goes down by  0.37. Similarly, if someone has a terrible credit history, then the odds of them defaulting on a loan goes down by 0.18. These results seem to be the opposite of what I expected. Odds of someone defaulting on a loan should rise if they have a poor or terrible credit history. 
  
  2. What do you think is going on here?
  Since the bank decided to conduct a case-control design, it used existing data to predict whether certain people will default on a loan, given other features. Since defaults are rare, the bank matched each default with similar sets of loans that had not defaulted. This means that they oversampled defaults relative to a random sample of loans in the bank's overall portfolio. 
  
  In a simpler way, imagine if we initially had a sample of 100 loans, of which 10 loans were defaulted. By oversampling defaults, instead of having a low default probability of 1/10, we now have a much higher probability. This method results in unreliable estimates since we can no longer draw reasonable conclusions about how a particular credit history may affect the default probability of an individual. This is why we are getting a negative coefficient for poor and terrible credit history while predicting loan defaults. 
  
  3. In light of what you see here, do you think this data set is appropriate for building a predictive model of defaults, if the purpose of the model is to screen prospective borrowers to classify them into "high" versus "low" probability of default?
  This dataset is therefore, not appropriate for building a predictive model of defaults. The case-control sample places a defaulted loan in a bag of loans of similar value without considering other features such as credit history, age, and savings. This means that we cannot draw useful conclusions about someone particular to their credit history and classify them into "high" versus "low" probability of default. 
  
  4. Why or why not---and if not, would you recommend any changes to the bank's sampling scheme?
  The bank should accord weights to the default loans in the sample.
  
```{r pressure, echo=FALSE}
credit <- read.csv("german_credit.csv")

str(credit)

tbl <- with(credit, table(history, Default))
tbl

##Bar plot for default probability by credit history
ggplot(as.data.frame(tbl), aes(factor(Default), Freq, fill = history)) + 
  geom_col(position = 'dodge') +
  labs(y = "Frequency", x = "Loan default", title ="Default probability by credit history", 
       caption = "0 indicates no default, 1 indicates default") + 
  scale_fill_manual(name = "Credit history", values=wes_palette(n=3, name="GrandBudapest1")) + 
  theme_bw() 
  
##Splitting data into train and test set
credit_split = initial_split(credit, prop=0.8)
credit_train = training(credit_split)
credit_test  = testing(credit_split)

logit_credit = glm(Default ~ duration + amount + installment + age 
                   + history + purpose + foreign, data=credit_train,
                   family='binomial')

credittest = mutate(credit_test, yhat = predict(logit_credit, credit_test,
                                                type='response'))

ggplot(credittest) + 
  geom_jitter(aes(x=factor(Default), y=yhat), width=0.1, alpha=0.2) + 
  labs(title="Test-set predicted probabilities", y = "P(Default | x)", x="Default?") + 
  stat_summary(aes(x=factor(Default), y=yhat), fun='mean', col='red', size=1)

coef(logit_credit) %>% round(2)
```


## Children and hotel reservations 

In this section, we will focus on building a predictive model for whether a booking at a hotel will have children on it. Oftentimes, parents fail to include their children when making reservations. Although this piece of information may seem trivial, being able to anticipate guests' needs are very important for hotel staff, so it is essential for management to know how many children are staying to better cater to families' needs and to keep supplies/inventory well-stocked.

Therefore, in this portion, we will focus on predicting the `children` variable.


### Model Building

```{r include=FALSE}
class_diag <- function(score, truth, model, test_set, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  ppv=tab[1,1]/colSums(tab)[1]
  rmse_ = rmse(model, test_set)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc, ppv, rmse_, auc, row.names = "Metrics"),4)
}
```


Using only the data in `hotels_dev.csv`, we will compare the out-of-sample performance of the following:

  1. baseline 1: a small model that uses only the `market_segment`, `adults`, `customer_type`, and `is_repeated_guest` variables as features.
  2. baseline 2: a big model that uses all the possible predictors except the `arrival_date` variable (main effects only)
  3. the best linear model you can build, including any engineered features that you can think of that improve the performance (interactions, features derived from time stamps, etc)



#### Baseline Model 1:

```{r echo=FALSE}
dev_hotels <- read.csv("hotels_dev.csv")

dev_split = initial_split(dev_hotels, prop=0.8)
dev_train = training(dev_split)
dev_test = testing(dev_split)

model1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, 
             data = dev_train, family = "binomial")
prob_model1 = predict(model1, newdata = dev_test, type= "response")

# out-of-sample accuracy
class_diag(prob_model1, dev_test$children, model1, dev_test, positive = 1)
```
After fitting the baseline model 1 to the training set and assessing out-of-sample accuracy, we see this model predicts with about 91% accuracy, but the area under the curve (AUC) is quite low. Although we would not consider "bad" model performance, the low AUC indicates that this is not the best model.


#### Baseline Model 2:

```{r echo=FALSE, warning=FALSE}
model2 = glm(children ~ . - arrival_date, 
             data = dev_train, family = "binomial")
prob_model2 = predict(model2, newdata = dev_test, type= "response")

# out-of-sample accuracy
class_diag(prob_model2, dev_test$children, model2, dev_test, positive = 1)
```
From the out-of-sample performance measures, we see that this model has both higher accuracy and a significantly higher AUC. Although the RMSE is higher for baseline model 2, I would still choose baseline model 2 over model 1 because both accuracy and AUC are improved. Furthermore, I have also calculated the value for positive predictive values (PPV), which are the proportion of true positives. In this case, the value for ppv is approximately 0.7, which means that, of the positive values predicted, about 70% are true positives.

#### Testing Best Linear Model:

In finding the "best" linear model, we have two different approaches. First, we manually build a linear model by adding variables and testing out-of-sample performance. Then, we built a model using forward selection. We, again, use out-of-sample performance to assess the models.

```{r echo=FALSE}
model3 = lm(children ~ . + customer_type:adults + adults:stays_in_weekend_nights + adults:stays_in_week_nights + stays_in_weekend_nights:stays_in_week_nights,
            data = dev_train[,!colnames(dev_train) %in% c("arrival_date")])
prob_model3 = predict(model3, newdata = dev_test, type= "response")

# out-of-sample accuracy
class_diag(prob_model3, dev_test$children, model3, dev_test, positive = 1)
```
In this first linear model, we include all variables (except `arrival_date`) as well as a few interaction terms (`adults:stays_in_weekend_nights`, `adults:stays_in_week_nights`, `stays_in_weekend_nights:stays_in_week_nights`). When comparing this linear model to Baseline Model 2, we see that the linear model has similar accuracy and AUC values as Model 2, but the RMSE value is significantly lower. This may have to do with differences between linear and logistic models. When we look at the PPV value, we see that the PPV for this linear model is slightly lower than the PPV in model 2.

Although the out-of-sample performance values may suggest this is also a good model, the logistic model is still preferred when predicting binary outcomes.


In the second linear model, we use forward selection to find a strong linear model, which gives us a regression where we regress `children` on:
`market_segment`, `customer_type`, `is_repeated_guest`, `adults`, `market_segment:adults`, `customer_type:adults`, and `market_segment:is_repeated_guest`


```{r include=FALSE}
lm0 = lm(children ~ 1, data=dev_train)
model4 = step(lm0, direction = "forward",
              scope = ~(market_segment + adults + customer_type + is_repeated_guest)^2)
prob_model4 = predict(model4, newdata = dev_test, type= "response")

```

```{r echo=FALSE}
# out-of-sample accuracy
class_diag(prob_model4, dev_test$children, model4, dev_test, positive = 1)
```
In the second linear model, accuracy and RMSE is comparable to the first linear model, but AUC drops significantly. Between the two linear models, the first linear model still outperforms the second.

Although the first linear model has similar accuracy, PPV, and AUC values, the logistic model is still preferred. Therefore, Baseline Model 2 is still the best model. 


### Model Validation: Step 1

The following is an ROC plot for Baseline Model 2, using the `hotels_val.csv` data.

```{r echo=FALSE, warning=FALSE}
val_hotels <- read.csv("hotels_val.csv")

phat_test_model2 = predict(model2, val_hotels, type = "response")
roc_model2 <- roc(val_hotels$children, phat_test_model2)
ggroc(roc_model2) + labs(title ="ROC Curve")

```

An ROC plots TPR vs. FPR. TPR is another name for sensitivity, while FPR is defined as 1-specificity, which explains why the numbers on the x-axis are flipped with 1 on the left and 0 on the right.



### Model Validation: Step 2

In this step, we create 20 folds of `hotels_val`, so each fold will have about 250 bookings. We conduct 20-fold cross-validation on Baseline Model 2.

For each fold, we will:
  1. Predict whether each booking will have children on it
  2. Sum up predicted probabilities for all bookings in the fold, giving an estimate of the expected number of bookings with children
  3. Compare the "expected" number of bookings with children vs. the actual number of bookings with children in that fold

```{r warning=FALSE, include=FALSE}
k = 20

folds <- rep(1:k, length.out = nrow(val_hotels))
diags <- NULL
pred_children <- c()
actual_children <- c()

i = 1
for (i in 1:k) {
  train <- val_hotels[folds != i, ]
  test <- val_hotels[folds == i, ]
  truth <- test$children
  
  fit = model2 = glm(children == 1 ~ ., 
             data = train[,!colnames(train) %in% c("arrival_date")], family = "binomial")

  probs <- predict(fit, newdata = test, type = "response")
  probs <- ifelse(probs > 0.5, 1, 0)
  diags <- rbind(diags, class_diag(probs, truth, fit, test, positive = 1))
  pred_children <- append(pred_children, sum(probs))
  actual_children <- append(actual_children, sum(truth))
}
cv_diags <- cbind(diags, pred_children, actual_children)
```

**The results for each fold is as follows:**

```{r echo=FALSE}
cv_diags
```


**Average of performance metrics across 20 folds:**

```{r echo=FALSE}
# average performance metrics across all folds
summarize_all(cv_diags, mean)
```

From the performance metrics, we see that accuracy stays above 90% on average, but PPV and AUC is relatively low. This means that this model could use further improvement to make better predictions.


---
title: "Exercises 2"
author: "Soo Jee Choi, Annie Nguyen, and Tarini Sudhakar"
date: "`r Sys.Date()`"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(rsample)
library(caret)
library(foreach)
library(modelr)
library(wesanderson)
library(pROC)
library(mosaicData)
library(mosaic)
library(Hmisc)
data(SaratogaHouses)
```

## Saratoga house prices
To know the appropriate taxation levels, we first need to ensure that we get our predicted market values of properties right. We conduct a "horse race" that is, compare the performance of two models on the Saratoga house prices. The dataset contains information such as lot size, age, bedrooms and more about 1728 houses in Saratoga, New York from 2006. 

We evaluate two models for predicting house prices: a linear model, and a K-nearest neighbour (KNN) regression model. For both models, we split our data into training and test sets with an 80-20 ratio. To test models, we compare their Root Mean Squared Error (RMSE). RMSE measures how large are the errors made by the model on the data, on average. Higher RMSE indicates poor predictive performance. There is further detail in the Appendices on the dataset and the models. 

There are two types of RMSE that we will looking at here: in-sample and out-of-sample. In-sample RMSE is when we test the model on the data it has already been trained on. Out-of-sample RMSE is when we run the model on test data that we split from the original dataset earlier. Out-of-sample RMSE is crucial because it tells us how well the model will perform on data it has not seen before.   

```{r echo=FALSE}
# Split into training and testing sets

saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
```

## Linear Model
```{r linear model, echo=FALSE}
linear_model = lm(price ~ lotSize + poly(age, 3) + poly(landValue,3) + livingArea + pctCollege:age + bedrooms + bathrooms + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir, data=saratoga_train)

len = length(coef(linear_model) %>% round(0))

# Predictions in sample
rmse(linear_model, saratoga_train)

# Predictions out of sample
rmse(linear_model, saratoga_test)
```
When we evaluate a single linear model train/test split, we get an in-sample RMSE of 57,603 and an out-of-sample RMSE of 55,614.

```{r linear model simulation, echo=FALSE}
linear_model = lm(price ~ lotSize + poly(age, 3) + poly(landValue,3) + livingArea + pctCollege:age + bedrooms + bathrooms + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir, data=saratoga_train)

rmse_simulation = do(25)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)

  linear_model = update(linear_model, data=saratoga_train)
  
  rmse(linear_model, data=saratoga_test)}
colMeans(rmse_simulation)

```
The average out-of-sample RMSE for 25 random linear model train/test splits is 64,567.

## KNN Model
```{r knn model, echo=FALSE, warning=FALSE}
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

# Standardize Variables
x_train = model.matrix(~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir -1, data=saratoga_train)
x_test = model.matrix(~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir -1, data=saratoga_test)

price_train = saratoga_train$price
price_test = saratoga_test$price

scale_train = apply(x_train, 2, sd)
x_tilde_train = scale(x_train, scale = scale_train)
x_tilde_test = scale(x_test, scale = scale_train)

# Convert matrices into dataframes
x_tilde_train_dataframe = as.data.frame(x_tilde_train)
x_tilde_test_dataframe = as.data.frame(x_tilde_test)
y_train_dataframe = as.data.frame(price_train)
y_test_dataframe = as.data.frame(price_test)

# Combine x and y datasets
training_set = cbind(y_train_dataframe,x_tilde_train_dataframe)
testing_set = cbind(y_test_dataframe,x_tilde_test_dataframe)

# KNN model
knn_model = knnreg(price_train ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms + newConstructionNo, data=training_set, k = 25)

# Predictions in sample
rmse(knn_model, training_set)

# Predictions out of sample
rmse(knn_model, testing_set)

```
In a single KNN regression model train/test split, we get an in-sample RMSE of about 60,000 and an out-of-sample RMSE of about 119,000.

```{r knn model simulation, echo=FALSE, warning=FALSE}
knn_model = knnreg(price_train ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms + newConstructionNo, data=training_set, k = 25)

rmse_simulation = do(25)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  x_train = model.matrix(~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir -1, data=saratoga_train)
  x_test = model.matrix(~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir -1, data=saratoga_test)

  price_train = saratoga_train$price
  price_test = saratoga_test$price

  scale_train = apply(x_train, 2, sd)
  x_tilde_train = scale(x_train, scale = scale_train)
  x_tilde_test = scale(x_test, scale = scale_train)

  # Convert matrices into dataframes
  x_tilde_train_dataframe = as.data.frame(x_tilde_train)
  x_tilde_test_dataframe = as.data.frame(x_tilde_test)
  y_train_dataframe = as.data.frame(price_train)
  y_test_dataframe = as.data.frame(price_test)

  # Combine x and y datasets
  training_set = cbind(y_train_dataframe,x_tilde_train_dataframe)
  testing_set = cbind(y_test_dataframe,x_tilde_test_dataframe)

  # KNN model
  knn_model = knnreg(price_train ~ lotSize + age + landValue + livingArea + bedrooms + bathrooms + rooms + newConstructionNo, data=training_set, k = 20)
  
  rmse(knn_model, data=testing_set)}
colMeans(rmse_simulation)

```
The average out-of-sample RMSE for 25 random KNN regression train/test splits is 121,277.

## What does it mean for predicting house prices? 

Overall, the linear model is the most well-suited for predicting house prices.  

We find that in a single linear model train/test split, we get an in-sample RMSE of 57,603 and an out-of-sample RMSE of 55,614. We standardize our variables before applying KNN and find using a single KNN regression model train/test split, results in an in-sample RMSE of 60,015 and an out-of-sample RMSE of 119,246. We can see that in a single train/test split, the in-sample RMSE of both models are similar (approximately 58,000 and 60,000 for the Linear and KNN models, respectively). 

However, the out-of-sample RMSE of the models were notably different at approximately 56,000 and 119,000. So while both models performed similarly using in-sample data, the linear model performed much better using the out-of-sample/testing data.

When measuring out-of-sample performance, there is random variation due to the particular choice of data points that end up in the train/test split sample. To address this issue, we average 25 different/random train/test split estimates of out-of-sample RMSE. By doing this, we see that the average out-of-sample RMSE of 25 random linear model train/test splits is 64,567, and the average out-of-sample RMSE of 25 random KNN regression train/test splits is 121,277. These results confirm our findings from the single train/test splits. So, we conclude that the linear regression model is better at achieving lower out-of-sample mean-squared error than the KNN model.

Note that this analysis relied on the best models we could produce using the data. There may exist a KNN model that outperforms the linear model we produced. But looking at the out-of-sample RMSE using single and average RMSE, that seems unlikely to be the case.

## Appendix 

### Appendix 1: Saratoga house prices dataset 

The dataset contains 1728 observations with 16 variables. 

```{r, echo=FALSE}
ls(SaratogaHouses)
```

### Appendix 2: Linear Model 
The linear model predicts price based on the following variables, and transformations and interactions between some of those variables. 

```{r, echo=FALSE}
appendix = lm(price ~ lotSize + poly(age, 3) + poly(landValue,3) + livingArea + pctCollege:age + bedrooms + bathrooms + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir, data=saratoga_train)

summary(appendix)
```
### Appendix 3: KNN Model 
The KNN model predicts a particular house price by using the average of K-nearest prices. For example, if K = 10, then the model will estimate a house price by using the average of its 10 closest data points. In our case, the optimal value of K is 25 since we have the lowest RMSE at that point. 

## Classification and retrospective sampling

In this part of the assignment, we want to be able to predict whether a person will default on their loan, based on factors such as credit history. What is unique about this problem is its dataset. Since the data available on loan defaults is small, the German bank decided to oversample its defaults by matching each default to a similar set of loans in the overall bank portfolio. Is this dataset appropriate for the model that we wish to build? Can it predict which of the bank's clients will default on a loan?  

What did we do to assess the data at hand? First, we wanted to see the default probability by credit history. Our results show that individuals with a good credit history have the highest probability of defaulting on a loan. Whereas, people with terrible credit history have the lowest. This seems counter intuitive since credit history indicates how well a borrower has repayed their debts. A good credit history should have a lower default probability. 

```{r pressure, echo=FALSE}
credit <- read.csv("german_credit.csv")

credit_summary = credit %>% 
  group_by(history) %>%
  summarise(mean(Default))

##Bar plot for default probability by credit history
dt <- ggplot(credit_summary, aes(history, `mean(Default)`), fill = history) + 
  geom_bar(stat = "identity", position = "dodge") +
  labs(y = "Probability", x = "Loan default", title ="Default probability by credit
       history", 
       caption = "0 indicates no default, 1 indicates default")  +
  theme_bw() + 
  scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))

dt
```

Nevertheless, if we build a logistic regression model around this dataset, regressing Default on duration, amount, installment, age, credit history, purpose, and foreign, we get the following results. 
```{r logreg, echo=FALSE}

##Logistic regression
logit_credit = glm(Default ~ duration + amount + installment + age 
                   + history + purpose + foreign, data=credit,
                   family='binomial')
summary(logit_credit)

coef(logit_credit) %>% round(2)
```

If someone has a poor credit history, then the chance of them defaulting on a loan goes down by 1.11. Similarly, if someone has a terrible credit history, then the odds of them defaulting on a loan goes down by 1.18. These results still are the opposite of what I expected. Odds of someone defaulting on a loan should rise if they have a poor or terrible credit history. 
  
### What is the bigger problem at play? 
  Since the bank decided to conduct a case-control design, it used existing data to predict whether certain people will default on a loan, given other features. Since defaults are rare, the bank matched each default with similar sets of loans that had not defaulted. This means that they oversampled defaults relative to a random sample of loans in the bank's overall portfolio. 
  
  In a simpler way, imagine if we initially had a sample of 100 loans, of which 10 loans were defaulted. By oversampling defaults, instead of having a low default probability of 1/10, we now have a much higher probability. This method results in unreliable estimates since we can no longer draw reasonable conclusions about how a particular credit history may affect the default probability of an individual. This is why we are getting a negative coefficient for poor and terrible credit history while predicting loan defaults. 
  

### How do we go ahead from here?  
  This dataset is not appropriate for building a predictive model of defaults. The case-control sample places a defaulted loan in a bag of loans of similar value. But we do not know if the bank considered other features such as credit history, age, and savings. This means that we cannot draw useful conclusions about someone particular to their credit history and classify them into "high" versus "low" probability of default. 
  
Instead of following this methodology, the bank can accord weights to the default loans in the sample. This will allow for their adequate representation in the sample that the bank has without diluting the validity of the estimates. 

## Children and hotel reservations 

In this section, we will focus on building a predictive model for whether a booking at a hotel will have children on it. Oftentimes, parents fail to include their children when making reservations. Although this piece of information may seem trivial, being able to anticipate guests' needs are very important for hotel staff, so it is essential for management to know how many children are staying to better cater to families' needs and to keep supplies/inventory well-stocked.

Therefore, in this portion, we will focus on predicting the `children` variable.

### Model Building

```{r include=FALSE}
class_diag <- function(score, truth, model, test_set, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  ppv=tab[1,1]/colSums(tab)[1]
  rmse_ = rmse(model, test_set)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc, ppv, rmse_, auc, row.names = "Metrics"),4)
}
```


Using only the data in `hotels_dev.csv`, we will compare the out-of-sample performance of the following:

  1. baseline 1: a small model that uses only the `market_segment`, `adults`, `customer_type`, and `is_repeated_guest` variables as features.
  2. baseline 2: a big model that uses all the possible predictors except the `arrival_date` variable (main effects only)
  3. the best linear model you can build, including any engineered features that you can think of that improve the performance (interactions, features derived from time stamps, etc)



#### Baseline Model 1:

```{r echo=FALSE}
dev_hotels <- read.csv("hotels_dev.csv")

dev_split = initial_split(dev_hotels, prop=0.8)
dev_train = training(dev_split)
dev_test = testing(dev_split)

model1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, 
             data = dev_train, family = "binomial")
prob_model1 = predict(model1, newdata = dev_test, type= "response")

# out-of-sample accuracy
class_diag(prob_model1, dev_test$children, model1, dev_test, positive = 1)
```
After fitting the baseline model 1 to the training set and assessing out-of-sample accuracy, we see this model predicts with about 91% accuracy, but the area under the curve (AUC) is quite low. Although we would not consider "bad" model performance, the low AUC indicates that this is not the best model.


#### Baseline Model 2:

```{r echo=FALSE, warning=FALSE}
model2 = glm(children ~ . - arrival_date, 
             data = dev_train, family = "binomial")
prob_model2 = predict(model2, newdata = dev_test, type= "response")

# out-of-sample accuracy
class_diag(prob_model2, dev_test$children, model2, dev_test, positive = 1)
```
From the out-of-sample performance measures, we see that this model has both higher accuracy and a significantly higher AUC. Although the RMSE is higher for baseline model 2, I would still choose baseline model 2 over model 1 because both accuracy and AUC are improved. Furthermore, I have also calculated the value for positive predictive values (PPV), which are the proportion of true positives. In this case, the value for ppv is approximately 0.7, which means that, of the positive values predicted, about 70% are true positives.

#### Testing Best Linear Model:

In finding the "best" linear model, we have two different approaches. First, we manually build a linear model by adding variables and testing out-of-sample performance. Then, we built a model using forward selection. We, again, use out-of-sample performance to assess the models.

```{r echo=FALSE}
model3 = lm(children ~ . + customer_type:adults + adults:stays_in_weekend_nights + adults:stays_in_week_nights + stays_in_weekend_nights:stays_in_week_nights,
            data = dev_train[,!colnames(dev_train) %in% c("arrival_date")])
prob_model3 = predict(model3, newdata = dev_test, type= "response")

# out-of-sample accuracy
class_diag(prob_model3, dev_test$children, model3, dev_test, positive = 1)
```
In this first linear model, we include all variables (except `arrival_date`) as well as a few interaction terms (`adults:stays_in_weekend_nights`, `adults:stays_in_week_nights`, `stays_in_weekend_nights:stays_in_week_nights`). When comparing this linear model to Baseline Model 2, we see that the linear model has similar accuracy and AUC values as Model 2, but the RMSE value is significantly lower. This may have to do with differences between linear and logistic models. When we look at the PPV value, we see that the PPV for this linear model is slightly lower than the PPV in model 2.

Although the out-of-sample performance values may suggest this is also a good model, the logistic model is still preferred when predicting binary outcomes.


In the second linear model, we use forward selection to find a strong linear model, which gives us a regression where we regress `children` on:
`market_segment`, `customer_type`, `is_repeated_guest`, `adults`, `market_segment:adults`, `customer_type:adults`, and `market_segment:is_repeated_guest`


```{r include=FALSE}
lm0 = lm(children ~ 1, data=dev_train)
model4 = step(lm0, direction = "forward",
              scope = ~(market_segment + adults + customer_type + is_repeated_guest)^2)
prob_model4 = predict(model4, newdata = dev_test, type= "response")

```

```{r echo=FALSE}
# out-of-sample accuracy
class_diag(prob_model4, dev_test$children, model4, dev_test, positive = 1)
```
In the second linear model, accuracy and RMSE is comparable to the first linear model, but AUC drops significantly. Between the two linear models, the first linear model still outperforms the second.

Although the first linear model has similar accuracy, PPV, and AUC values, the logistic model is still preferred. Therefore, Baseline Model 2 is still the best model. 


### Model Validation: Step 1

The following is an ROC plot for Baseline Model 2, using the `hotels_val.csv` data.

```{r echo=FALSE, warning=FALSE}
val_hotels <- read.csv("hotels_val.csv")

phat_test_model2 = predict(model2, val_hotels, type = "response")
roc_model2 <- roc(val_hotels$children, phat_test_model2)
ggroc(roc_model2) + labs(title ="ROC Curve")

```

An ROC plots TPR vs. FPR. TPR is another name for sensitivity, while FPR is defined as 1-specificity, which explains why the numbers on the x-axis are flipped with 1 on the left and 0 on the right.



### Model Validation: Step 2

In this step, we create 20 folds of `hotels_val`, so each fold will have about 250 bookings. We conduct 20-fold cross-validation on Baseline Model 2.

For each fold, we will:
  1. Predict whether each booking will have children on it
  2. Sum up predicted probabilities for all bookings in the fold, giving an estimate of the expected number of bookings with children
  3. Compare the "expected" number of bookings with children vs. the actual number of bookings with children in that fold

```{r warning=FALSE, include=FALSE}
k = 20

folds <- rep(1:k, length.out = nrow(val_hotels))
diags <- NULL
pred_children <- c()
actual_children <- c()

i = 1
for (i in 1:k) {
  train <- val_hotels[folds != i, ]
  test <- val_hotels[folds == i, ]
  truth <- test$children
  
  fit = model2 = glm(children == 1 ~ ., 
             data = train[,!colnames(train) %in% c("arrival_date")], family = "binomial")

  probs <- predict(fit, newdata = test, type = "response")
  probs <- ifelse(probs > 0.5, 1, 0)
  diags <- rbind(diags, class_diag(probs, truth, fit, test, positive = 1))
  pred_children <- append(pred_children, sum(probs))
  actual_children <- append(actual_children, sum(truth))
}
cv_diags <- cbind(diags, pred_children, actual_children)
```

**The results for each fold is as follows:**

```{r echo=FALSE}
cv_diags
```


**Average of performance metrics across 20 folds:**

```{r echo=FALSE}
# average performance metrics across all folds
summarize_all(cv_diags, mean)
```

From the performance metrics, we see that accuracy stays above 90% on average, but PPV and AUC is relatively low. This means that this model could use further improvement to make better predictions.


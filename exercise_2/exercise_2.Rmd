---
title: "Exercises 2"
author: "Soo Jee, Annie Nguyen, and Tarini Sudhakar"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(rsample)
library(caret)
library(foreach)
library(modelr)
library(wesanderson)
library(pROC)
```

## Saratoga house prices


```{r cars}
summary(cars)
```

## Classification and retrospective sampling
What do you notice about the history variable vis-a-vis predicting defaults? What do you think is going on here? In light of what you see here, do you think this data set is appropriate for building a predictive model of defaults, if the purpose of the model is to screen prospective borrowers to classify them into "high" versus "low" probability of default? Why or why not---and if not, would you recommend any changes to the bank's sampling scheme?

  1. What do you notice about the history variable vis-a-vis predicting defaults? 
  If someone has a poor credit history, then the chance of them defaulting on a loan goes down by  0.37. Similarly, if someone has a terrible credit history, then the odds of them defaulting on a loan goes down by 0.18. These results seem to be the opposite of what I expected. Odds of someone defaulting on a loan should rise if they have a poor or terrible credit history. 
  
  2. What do you think is going on here?
  Since the bank decided to conduct a case-control design, it used existing data to predict whether certain people will default on a loan, given other features. Since defaults are rare, the bank matched each default with similar sets of loans that had not defaulted. This means that they oversampled defaults relative to a random sample of loans in the bank's overall portfolio. 
  
  In a simpler way, imagine if we initially had a sample of 100 loans, of which 10 loans were defaulted. By oversampling defaults, instead of having a low default probability of 1/10, we now have a much higher probability. This method results in unreliable estimates since we can no longer draw reasonable conclusions about how a particular credit history may affect the default probability of an individual. This is why we are getting a negative coefficient for poor and terrible credit history while predicting loan defaults. 
  
  3. In light of what you see here, do you think this data set is appropriate for building a predictive model of defaults, if the purpose of the model is to screen prospective borrowers to classify them into "high" versus "low" probability of default?
  This dataset is therefore, not appropriate for building a predictive model of defaults. The case-control sample places a defaulted loan in a bag of loans of similar value without considering other features such as credit history, age, and savings. This means that we cannot draw useful conclusions about someone particular to their credit history and classify them into "high" versus "low" probability of default. 
  
  4. Why or why not---and if not, would you recommend any changes to the bank's sampling scheme?
  The bank should accord weights to the default loans in the sample.
  
```{r pressure, echo=FALSE}
credit <- read.csv("german_credit.csv")

str(credit)

tbl <- with(credit, table(history, Default))
tbl

##Bar plot for default probability by credit history
ggplot(as.data.frame(tbl), aes(factor(Default), Freq, fill = history)) + 
  geom_col(position = 'dodge') +
  labs(y = "Frequency", x = "Loan default", title ="Default probability by credit history", 
       caption = "0 indicates no default, 1 indicates default") + 
  scale_fill_manual(name = "Credit history", values=wes_palette(n=3, name="GrandBudapest1")) + 
  theme_bw() 
  
##Splitting data into train and test set
credit_split = initial_split(credit, prop=0.8)
credit_train = training(credit_split)
credit_test  = testing(credit_split)

logit_credit = glm(Default ~ duration + amount + installment + age 
                   + history + purpose + foreign, data=credit_train,
                   family='binomial')

credittest = mutate(credit_test, yhat = predict(logit_credit, credit_test,
                                                type='response'))

ggplot(credittest) + 
  geom_jitter(aes(x=factor(Default), y=yhat), width=0.1, alpha=0.2) + 
  labs(title="Test-set predicted probabilities", y = "P(Default | x)", x="Default?") + 
  stat_summary(aes(x=factor(Default), y=yhat), fun='mean', col='red', size=1)

coef(logit_credit) %>% round(2)
```

## Children and hotel reservations 

### Model Building

```{r}
class_diag <- function(score, truth, model, test_set, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  rmse_ = rmse(model, test_set)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc, rmse_, auc),4)
}

```


```{r}
dev_hotels <- read.csv("hotels_dev.csv")

dev_split = initial_split(dev_hotels, prop=0.8)
dev_train = training(dev_split)
dev_test = testing(dev_split)

model1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, 
             data = dev_train, family = "binomial")
prob_model1 = predict(model1, newdata = dev_test, type= "response")

model2 = glm(children ~ . - arrival_date, 
             data = dev_train, family = "binomial")
prob_model2 = predict(model2, newdata = dev_test, type= "response")

# tested normal linear model w/ all variable, except arrival_date
# also used forward selection to build model 4, but model 3 still outperforms
# according to RMSE
model3 = lm(children ~ . - arrival_date,
            data = dev_train)
prob_model3 = predict(model3, newdata = dev_test, type= "response")

lm0 = lm(children ~ 1, data=dev_train)
model4 = step(lm0, direction = "forward",
              scope = ~(market_segment + adults + customer_type + is_repeated_guest)^2)
prob_model4 = predict(model4, newdata = dev_test, type= "response")


# out-of-sample accuracy
class_diag(prob_model1, dev_test$children, model1, dev_test, positive = 1)
class_diag(prob_model2, dev_test$children, model2, dev_test, positive = 1)
class_diag(prob_model3, dev_test$children, model3, dev_test, positive = 1)
class_diag(prob_model4, dev_test$children, model4, dev_test, positive = 1)
```


### Model Validation: Step 1

```{r}
val_hotels <- read.csv("hotels_val.csv")

# take out model 1
phat_test_model1 = predict(model1, val_hotels, type = "response")
roc_model1 <- roc(val_hotels$children, phat_test_model1)
ggroc(roc_model1)

phat_test_model2 = predict(model2, val_hotels, type = "response")
roc_model2 <- roc(val_hotels$children, phat_test_model2)
ggroc(roc_model2)

phat_test_model3 = predict(model3, val_hotels, type = "response")
roc_model3 <- roc(val_hotels$children, phat_test_model3)
ggroc(roc_model3)
```




### Model Validation: Step 2

```{r}
k = 20

folds <- rep(1:k, length.out = nrow(val_hotels))
diags <- NULL
pred_children <- c()
actual_children <- c()

i = 1
for (i in 1:k) {
  train <- val_hotels[folds != i, ]
  test <- val_hotels[folds == i, ]
  truth <- test$children
  
  fit = lm(formula = children ~ market_segment + customer_type + adults + 
    is_repeated_guest + market_segment:adults + customer_type:adults + 
    market_segment:is_repeated_guest + market_segment:customer_type, 
    data = train)

  probs <- predict(fit, newdata = test, type = "response")
  probs <- ifelse(probs > 0.5, 1, 0)
  diags <- rbind(diags, class_diag(probs, truth, fit, test, positive = 1))
  pred_children <- append(pred_children, sum(probs))
  actual_children <- append(actual_children, sum(truth))
}
cv_diags <- cbind(diags, pred_children, actual_children)
cv_diags

summarize_all(cv_diags, mean)
```


```{r}
k = 20

folds <- rep(1:k, length.out = nrow(val_hotels))
diags <- NULL
pred_children <- c()
actual_children <- c()

i = 1
for (i in 1:k) {
  train <- val_hotels[folds != i, ]
  test <- val_hotels[folds == i, ]
  truth <- test$children
  
  fit = model2 = glm(children == 1 ~ ., 
             data = train[,!colnames(train) %in% c("arrival_date")], family = "binomial")

  probs <- predict(fit, newdata = test, type = "response")
  probs <- ifelse(probs > 0.5, 1, 0)
  diags <- rbind(diags, class_diag(probs, truth, fit, test, positive = 1))
  pred_children <- append(pred_children, sum(probs))
  actual_children <- append(actual_children, sum(truth))
}
cv_diags <- cbind(diags, pred_children, actual_children)
cv_diags

# average performance metrics across all folds
summarize_all(cv_diags, mean)
```



```{r}
# refer to loadhou.R
K_folds = 20

# pipline 1:
cv_val = val_hotels %>% mutate(fold_id = rep(1:K_folds, length=nrow(val_hotels)) %>% sample)
```

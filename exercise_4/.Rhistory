library(foreach)
library(mosaic)
library(rsample)
library(modelr)
clust2 = kmeanspp(X, k=6, nstart=25)
clust2 = kmeanspp(X, k=6, nstart=25)
install.packages("LICORS")
install.packages('LICORS', repos='https://cran.r-project.org/src/contrib/Archive/LICORS/LICORS_0.2.0.tar.gz', dependencies=TRUE)
install.packages("ClusterR")
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
cache = TRUE
)
library(tidyverse)
library(ClusterR) # new for kmeans++
library(foreach)
library(mosaic)
library(rsample)
library(modelr)
#Clean data
X = social_marketing[,-c(1, 2, 6, 36, 37)]
# Center and scale the data
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
# Run k-means with 6 clusters and 25 starts
clust1 = kmeans(X, 6, nstart=25)
# What are the clusters?
clust1$center  # not super helpful
clust1$center[1,]*sigma + mu
clust1$center[2,]*sigma + mu
clust1$center[4,]*sigma + mu
# Which observations are in which clusters?
which(clust1$cluster == 1)
which(clust1$cluster == 2)
which(clust1$cluster == 3)
which(clust1$cluster == 4)
which(clust1$cluster == 5)
# qplot is in the ggplot2 library
qplot(food, family, data=social_marketing, color=factor(clust1$cluster))
qplot(cooking, personal_fitness, data=social_marketing, color=factor(clust1$cluster))
# ggplot 2 equivalent
ggplot(social_marketing) +
geom_point(aes(food, family, color=factor(clust1$cluster)))
# Using kmeans++ initialization
clust2 = KMeans_rcpp(X, clusters=6, num_init=25)
clust2$centroids[1,]*sigma + mu
clust2$centroids[2,]*sigma + mu
clust2$centroids[4,]*sigma + mu
# Which cars are in which clusters?
names(clust2$cluster) = rownames(X)
which(clust2$cluster == 1)
which(clust2$cluster == 2)
which(clust2$cluster == 3)
# Compare versus within-cluster average distances from the first run
clust1$withinss
clust2$withinss
sum(clust1$withinss)
sum(clust2$withinss)
clust1$tot.withinss
clust2$tot.withinss
clust1$betweenss
clust2$betweenss
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
cache = TRUE
)
library(tidyverse)
library(ClusterR) # new for kmeans++
library(foreach)
library(mosaic)
library(rsample)
library(modelr)
library(randomForest)
library(splines)
library(pdp)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=5, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=7, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=10, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=15, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=20, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=25, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=27, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
qplot(current_events, travek, data=social_marketing, color=factor(clust1$cluster))
qplot(current_events, travel, data=social_marketing, color=factor(clust1$cluster))
qplot(photo_sharing, travel, data=social_marketing, color=factor(clust1$cluster))
corr_matrix <- cor(X)
ggcorrplot(corr_matrix)
install.packages("ggcorrplot")
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
cache = TRUE
)
library(tidyverse)
library(ClusterR) # new for kmeans++
library(foreach)
library(mosaic)
library(rsample)
library(modelr)
library(randomForest)
library(splines)
library(pdp)
library(ggcorrplot)
corr_matrix <- cor(X)
ggcorrplot(corr_matrix)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=5, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=7, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=10, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
#Clean data
X = social_marketing[,-c(1, 2, 6, 36, 37)]
# Center and scale the data
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
#Correlation matrix
corr_matrix <- cor(X)
ggcorrplot(corr_matrix)
# Run k-means with 6 clusters and 25 starts
clust1 = kmeans(X, 6, nstart=25)
# What are the clusters?
clust1$center  # not super helpful
clust1$center[1,]*sigma + mu
clust1$center[2,]*sigma + mu
clust1$center[4,]*sigma + mu
# Which observations are in which clusters?
which(clust1$cluster == 1)
which(clust1$cluster == 2)
which(clust1$cluster == 3)
which(clust1$cluster == 4)
which(clust1$cluster == 5)
# qplot is in the ggplot2 library
qplot(food, family, data=social_marketing, color=factor(clust1$cluster))
qplot(cooking, personal_fitness, data=social_marketing, color=factor(clust1$cluster))
qplot(photo_sharing, travel, data=social_marketing, color=factor(clust1$cluster))
# ggplot 2 equivalent
ggplot(social_marketing) +
geom_point(aes(food, family, color=factor(clust1$cluster)))
# Using kmeans++ initialization
clust2 = KMeans_rcpp(X, clusters=6, num_init=25)
clust2$centroids[1,]*sigma + mu
clust2$centroids[2,]*sigma + mu
clust2$centroids[4,]*sigma + mu
# Which cars are in which clusters?
names(clust2$cluster) = rownames(X)
which(clust2$cluster == 1)
which(clust2$cluster == 2)
which(clust2$cluster == 3)
which(clust2$cluster == 4)
which(clust2$cluster == 5)
which(clust2$cluster == 6)
# Compare versus within-cluster average distances from the first run
clust1$withinss
clust2$withinss
sum(clust1$withinss)
sum(clust2$withinss)
clust1$tot.withinss
clust2$tot.withinss
clust1$betweenss
clust2$betweenss
#Clean data
X = social_marketing[,-c(1, 2, 6, 36, 37)]
# Center and scale the data
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
#Correlation matrix
corr_matrix <- cor(X)
ggcorrplot(corr_matrix)
ggcorrplot(corr_matrix, type = “lower”, outline.color = “white”)
ggcorrplot(corr_matrix, hc.order = TRUE, type = “lower”, outline.color = “white”)
#Clean data
X = social_marketing[,-c(1, 2, 6, 36, 37)]
# Center and scale the data
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
#Correlation matrix
corr_matrix <- cor(X)
ggcorrplot(corr_matrix, outline.color = “white”)
#Clean data
X = social_marketing[,-c(1, 2, 6, 36, 37)]
# Center and scale the data
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
#Correlation matrix
corr_matrix <- cor(X)
ggcorrplot(corr_matrix)
corr_matrix
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=10, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
plot(pc_sm)
round(pc_sm$rotation[,1:10],2)
getwd()
# Now run PCA on the weather data
pc_sm = prcomp(X, rank=10, center = TRUE, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
plot(pc_sm)
# Now run PCA on the social marketing data
pc_sm = prcomp(X, rank=15, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
plot(pc_sm)
# Now run PCA on the social marketing data
pc_sm = prcomp(X, rank=10, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
plot(pc_sm)
# Now run PCA on the social marketing data
pc_sm = prcomp(X, center = TRUE, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
plot(pc_sm)
# Now run PCA on the social marketing data
pc_sm = prcomp(X, rank=10, center = TRUE, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
plot(pc_sm)
pr_var <-  pc_sm$sdev ^ 2
pve <- pr_var / sum(pc_sm)
pr_var <-  pc_sm$sdev ^ 2
pve <- pr_var / sum(pr_var)
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = 'b')
# Now run PCA on the social marketing data
pc_sm = prcomp(X, rank=19, center = TRUE, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_sm$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_sm)
plot(pc_sm)
pr_var <-  pc_sm$sdev ^ 2
pve <- pr_var / sum(pr_var)
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = 'b')
plot(cumsum, xlab = "Principal Component", ylab = "Cummulative Proportion of Variance Explained", ylim = c(0,1), type = 'b')
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cummulative Proportion of Variance Explained", ylim = c(0,1), type = 'b')
round(pc_sm$rotation[,1:10],2)
pc_sm$x
View(pc_sm)
# Checking loadings on PCA
round(pc_sm$rotation[,1:10],2)
loadings_summary = pc_sm$rotation %>%
as.data.frame() %>%
rownames_to_column('Category')
loadings_summary
loadings_summary %>%
select(Category, PC1) %>%
arrange(desc(PC1))
loadings_summary %>%
select(Category, PC2) %>%
arrange(desc(PC2))
loadings_summary %>%
select(Category, PC1) %>%
arrange(desc(PC1))
loadings_summary %>%
select(Category, PC2) %>%
arrange(desc(PC2))
loadings_summary %>%
select(Category, PC3) %>%
arrange(desc(PC3))
loadings_summary %>%
select(Category, PC2) %>%
arrange(aesc(PC2))
loadings_summary %>%
select(Category, PC2) %>%
arrange(asc(PC2))
loadings_summary %>%
select(Category, PC2) %>%
arrange(PC2)
loadings_summary %>%
select(Category, PC1) %>%
arrange(PC1)
loadings_summary %>%
select(Category, PC3) %>%
arrange(desc(PC3))
loadings_summary %>%
select(Category, PC3) %>%
arrange(PC3)
#PC4
loadings_summary %>%
select(Category, PC4) %>%
arrange(desc(PC4))
loadings_summary %>%
select(Category, PC4) %>%
arrange(PC4)
#PC5
loadings_summary %>%
select(Category, PC5) %>%
arrange(desc(PC5))
loadings_summary %>%
select(Category, PC5) %>%
arrange(PC5)
loadings_summary %>%
select(Category, PC6) %>%
arrange(desc(PC6))
loadings_summary %>%
select(Category, PC6) %>%
arrange(PC6)
loadings_summary %>%
select(Category, PC7) %>%
arrange(desc(PC7))
loadings_summary %>%
select(Category, PC7) %>%
arrange(PC7)
loadings_summary %>%
select(Category, PC8) %>%
arrange(desc(PC8))
loadings_summary %>%
select(Category, PC8) %>%
arrange(PC8)
loadings_summary %>%
select(Category, PC9) %>%
arrange(desc(PC9))
loadings_summary %>%
select(Category, PC9) %>%
arrange(PC9)
loadings_summary %>%
select(Category, PC10) %>%
arrange(desc(PC10))
loadings_summary %>%
select(Category, PC10) %>%
arrange(PC10)
# Checking loadings on PCA
round(pc_sm$rotation[,1:10],2)
# create a tidy summary of the loadings
loadings_summary = pc_sm$rotation %>%
as.data.frame() %>%
rownames_to_column('Category')
#PC1
loadings_summary %>%
select(Category, PC1) %>%
arrange(desc(PC1))
#PC2
loadings_summary %>%
select(Category, PC2) %>%
arrange(desc(PC2))
#PC3
loadings_summary %>%
select(Category, PC3) %>%
arrange(desc(PC3))
#PC4
loadings_summary %>%
select(Category, PC4) %>%
arrange(desc(PC4))
#PC5
loadings_summary %>%
select(Category, PC5) %>%
arrange(desc(PC5))
#PC6
loadings_summary %>%
select(Category, PC6) %>%
arrange(desc(PC6))
#PC7
loadings_summary %>%
select(Category, PC7) %>%
arrange(desc(PC7))
#PC8
loadings_summary %>%
select(Category, PC8) %>%
arrange(desc(PC8))
#PC9
loadings_summary %>%
select(Category, PC9) %>%
arrange(desc(PC9))
#PC10
loadings_summary %>%
select(Category, PC10) %>%
arrange(desc(PC10))
qplot(scores[,1], scores[,2], color=loadings_summary$Category, xlab='Component 1', ylab='Component 2')
qplot(Category[,1], Category[,2], color=loadings_summary$Category, xlab='Component 1', ylab='Component 2')
Y <- X %>%
as.data.frame() %>%
rownames_to_column('Category')
View(Y)
Y <- cbind(X, scores)
scores = pc_sm$x
Y <- cbind(X, scores)
View(Y)
qplot(PC1, PC2, color=Y, xlab='Component 1', ylab='Component 2')
ggplot(Y) +
geom_point(aes(PC1, PC2))
scores = pc_sm$x
Y <- as.data.frame(cbind(X, scores))
ggplot(Y) +
geom_point(aes(PC1, PC2))
barplot(table(clust1$cluster), col="#336699")
pca <- prcomp(X, scale=TRUE) #principle component analysis
pca_data <- mutate(fortify(pca), col=clust1$cluster)
pca <- as.data.frame(prcomp(X, scale=TRUE)) #principle component analysis
pca <- prcomp(X, scale=TRUE) #principle component analysis
pca_data <- mutate(as.data.frame(pca), col=clust1$cluster)
pca <- prcomp(X, scale=TRUE) #principle component analysis
pca_data <- mutate(pca, col=clust1$cluster)
scores = pc_sm$x
Y <- as.data.frame(cbind(X, scores))
ggplot(Y) + geom_point(aes(x=PC1, y=PC2, fill=factor(clust1$cluster)),
size=3, col="#7f7f7f", shape=21) + theme_bw(base_family="Helvetica")
ggplot(Y) + geom_point(aes(x=PC1, y=PC3, fill=factor(clust1$cluster)),
size=3, col="#7f7f7f", shape=21) + theme_bw(base_family="Helvetica")
which(clust1$cluster == 5)
ggplot(Y) + geom_point(aes(x=PC1, y=PC3, fill=factor(clust1$cluster)),
size=3, col="#7f7f7f", shape=21) + theme_bw(base_family="Helvetica")
ggplot(Y) + geom_point(aes(x=PC1, y=PC4, fill=factor(clust1$cluster)),
size=3, col="#7f7f7f", shape=21) + theme_bw(base_family="Helvetica")
ggplot(Y) + geom_point(aes(x=PC1, y=PC9, fill=factor(clust1$cluster)),
size=3, col="#7f7f7f", shape=21) + theme_bw(base_family="Helvetica")
